import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

lr = LinearRegression()
nb = MultinomialNB()
rf = RandomForestClassifier(random_state=1)
svm = svm.SVC()
gb = GradientBoostingClassifier(n_estimators=10)
dt = DecisionTreeClassifier(random_state=0)
mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=0)
logr = LogisticRegression(random_state=0)

df = pd.read_csv("C:/Users/CC-105/Desktop/Housing.csv")
#print(df.head(10))

x = df.drop(["medv","chas","rad","b"], axis=1)
g = pd.cut(df["medv"],3,labels=['0','1','2'])
le = LabelEncoder()
le.fit(g)
y = le.transform(g)
# y = df["medv"]




# print(x)
# print(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size=0.3)


# train = lr.fit(x_train,y_train)
# pred = lr.predict(x_test)

# print(pred)
# print(mean_squared_error(y_test,pred))

train1 = lr.fit(x_train,y_train)
pred1 = lr.predict(x_test)


print(pred1)
print(mean_squared_error(y_test,pred1))
